import os
import shutil
from PIL import Image
from PIL.ExifTags import TAGS
import datetime
import subprocess
import concurrent.futures
import logging
import time
from functools import lru_cache

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

def setup_logging(verbose=False):
    """é…ç½®æ—¥å¿—çº§åˆ«"""
    log_level = logging.DEBUG if verbose else logging.INFO
    logger.setLevel(log_level)

def get_cached_file_timestamp(filepath):
    """è·å–å¸¦ç¼“å­˜çš„æ–‡ä»¶æ—¶é—´æˆ³ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰"""
    try:
        # ç¼“å­˜æœ€è¿‘è®¿é—®çš„æ–‡ä»¶æ—¶é—´æˆ³ï¼ˆæ¯ä¸ªè·¯å¾„32ä¸ªç¼“å­˜é¡¹ï¼‰
        return get_cached_file_timestamp.cache.get(filepath)
    except KeyError:
        timestamp = os.path.getmtime(filepath)
        get_cached_file_timestamp.cache[filepath] = timestamp
        return timestamp

# ä¸ºæ—¶é—´æˆ³å‡½æ•°è®¾ç½®ç¼“å­˜ï¼ˆæœ€å¤šç¼“å­˜1024ä¸ªæ–‡ä»¶çš„æ—¶é—´æˆ³ï¼‰
get_cached_file_timestamp.cache = {}
get_cached_file_timestamp.cache = lru_cache(maxsize=1024)(lambda filepath: os.path.getmtime(filepath))

def get_image_exif_date(image_path):
    """ä»å›¾ç‰‡EXIFè·å–æ—¥æœŸï¼ˆå•ç‹¬å‡½æ•°ä¾¿äºé‡ç”¨å’Œä¼˜åŒ–ï¼‰"""
    try:
        with Image.open(image_path) as img:
            exif_data = img.getexif()
            
            if not exif_data:
                return None
            
            # é«˜æ•ˆæŸ¥æ‰¾æ—¥æœŸæ ‡ç­¾
            date_tag_ids = [tag_id for tag_id, tag_name in TAGS.items() 
                          if tag_name in ['DateTimeOriginal', 'DateTimeDigitized', 'DateTime']]
            
            date_str = None
            for tag_id in date_tag_ids:
                value = exif_data.get(tag_id)
                if value:
                    try:
                        # å°è¯•è§£æå¸¸è§æ—¥æœŸæ ¼å¼
                        date_str = value.strip()
                        if ':' in date_str:
                            date_part = date_str.split()[0].replace(":", "-")
                            return datetime.datetime.strptime(date_part, "%Y-%m-%d").date()
                    except (ValueError, TypeError):
                        continue
    except Exception as e:
        logger.debug(f"Error reading EXIF from {os.path.basename(image_path)}: {str(e)}")
    return None

def get_video_metadata_date(video_path):
    """è·å–è§†é¢‘æ–‡ä»¶çš„æ—¥æœŸå…ƒæ•°æ®"""
    try:
        cmd = [
            'ffprobe', '-v', 'error',
            '-show_entries', 'format_tags=creation_time',
            '-of', 'default=nw=1:nk=1',
            video_path
        ]
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=5)
        if result.returncode == 0:
            date_str = result.stdout.strip()
            if date_str:
                # å°è¯•å¤šç§å¸¸ç”¨æ—¥æœŸæ ¼å¼
                for fmt in ("%Y-%m-%dT%H:%M:%S", "%Y%m%d", "%Y-%m-%d %H:%M:%S"):
                    try:
                        # åˆ†å‰²å¯èƒ½å­˜åœ¨çš„æ¯«ç§’éƒ¨åˆ†
                        clean_date = date_str.split('.')[0]
                        dt = datetime.datetime.strptime(clean_date, fmt)
                        return dt.date()
                    except ValueError:
                        continue
    except (FileNotFoundError, subprocess.CalledProcessError, subprocess.TimeoutExpired) as e:
        logger.debug(f"è§†é¢‘å…ƒæ•°æ®è·å–å¤±è´¥ï¼ˆ{os.path.basename(video_path)}ï¼‰ï¼š{str(e)}")
    return None

def get_media_date(media_path):
    """ä¼˜åŒ–åçš„åª’ä½“æ—¥æœŸè·å–å‡½æ•°"""
    # å›¾ç‰‡æ–‡ä»¶ä¼˜å…ˆå°è¯•EXIF
    lower_path = media_path.lower()
    
    if any(ext in lower_path for ext in ('.jpg', '.jpeg', '.png', '.heic', '.tiff', '.nef', '.cr2', '.arw', '.dng')):
        exif_date = get_image_exif_date(media_path)
        if exif_date:
            return exif_date
        
    # è§†é¢‘æ–‡ä»¶å°è¯•è·å–å…ƒæ•°æ®
    if any(ext in lower_path for ext in ('.mp4', '.mov', '.avi', '.mkv', '.flv', '.wmv', '.3gp', '.m4v', '.mts')):
        video_date = get_video_metadata_date(media_path)
        if video_date:
            return video_date
    
    # æœ€åä½¿ç”¨ç¼“å­˜çš„æ–‡ä»¶ä¿®æ”¹æ—¶é—´
    timestamp = get_cached_file_timestamp(media_path)
    return datetime.datetime.fromtimestamp(timestamp).date()

def calculate_target_path(file_info, target_base_dir):
    """è®¡ç®—æ–‡ä»¶çš„ç›®æ ‡è·¯å¾„ï¼ˆé¿å…é‡å¤å¤„ç†ï¼‰"""
    filename, source_path = file_info
    media_date = get_media_date(source_path)
    date_folder = media_date.strftime("%Y-%m-%d")
    target_dir = os.path.join(target_base_dir, date_folder)
    
    # åˆ›å»ºç›®æ ‡ç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    os.makedirs(target_dir, exist_ok=True)
    
    # å¤„ç†æ–‡ä»¶åå†²çª
    new_path = os.path.join(target_dir, filename)
    if os.path.exists(new_path):
        base, ext = os.path.splitext(filename)
        counter = 1
        while True:
            new_filename = f"{base}_{counter}{ext}"
            new_path = os.path.join(target_dir, new_filename)
            if not os.path.exists(new_path):
                break
            counter += 1
    
    return (source_path, new_path, date_folder)

def process_file(file_task):
    """å¤„ç†å•ä¸ªæ–‡ä»¶ï¼ˆç§»åŠ¨æ“ä½œï¼‰"""
    source_path, new_path, date_folder = file_task
    filename = os.path.basename(source_path)
    
    try:
        shutil.move(source_path, new_path)
        new_filename = os.path.basename(new_path)
        logger.info(f"âœ“ å·²ç§»åŠ¨: {filename} -> {date_folder}/{new_filename}")
        return True
    except Exception as e:
        logger.error(f"âœ— ç§»åŠ¨å¤±è´¥: {filename} - é”™è¯¯: {str(e)}")
        return False

def organize_media(source_dir, target_base_dir=None, verbose=False, max_workers=None):
    """ä¸»å‡½æ•°ï¼šæŒ‰æ—¥æœŸæ•´ç†åª’ä½“æ–‡ä»¶ï¼ˆå›¾ç‰‡+è§†é¢‘ï¼‰"""
    setup_logging(verbose)
    start_time = time.time()
    
    if target_base_dir is None:
        target_base_dir = source_dir
    
    # æ”¯æŒçš„åª’ä½“æ ¼å¼ï¼ˆä½¿ç”¨å…ƒç»„æé«˜æŸ¥æ‰¾æ•ˆç‡ï¼‰
    valid_extensions = (
        # å›¾ç‰‡æ ¼å¼
        '.jpg', '.jpeg', '.png', '.heic', '.tiff', '.nef', '.cr2', '.arw', '.dng',
        # è§†é¢‘æ ¼å¼
        '.mp4', '.mov', '.avi', '.mkv', '.flv', '.wmv', '.3gp', '.m4v', '.mts'
    )
    
    # æ”¶é›†åª’ä½“æ–‡ä»¶åˆ—è¡¨ï¼ˆé¢„è¿‡æ»¤ï¼‰
    media_files = []
    skipped_files = []
    
    logger.info(f"å¼€å§‹æ‰«æç›®å½•: {os.path.abspath(source_dir)}")
    file_counter = 0
    for filename in os.listdir(source_dir):
        file_counter += 1
        filepath = os.path.join(source_dir, filename)
        
        if os.path.isdir(filepath):
            continue
        
        if filename.lower().endswith(valid_extensions):
            media_files.append((filename, filepath))
        else:
            skipped_files.append(filepath)
    
    # å¤šçº¿ç¨‹å¤„ç†ä»»åŠ¡
    worker_count = max_workers or max(2, os.cpu_count() * 2)  # é»˜è®¤CPUæ ¸å¿ƒæ•°Ã—2
    logger.info(f"å…±æ‰¾åˆ° {len(media_files)} ä¸ªåª’ä½“æ–‡ä»¶ï¼Œä½¿ç”¨ {worker_count} ä¸ªå¹¶è¡Œä»»åŠ¡")
    
    # ç¬¬ä¸€æ­¥ï¼šæ‰¹é‡è®¡ç®—ç›®æ ‡è·¯å¾„
    with concurrent.futures.ThreadPoolExecutor(max_workers=min(worker_count, 12)) as executor:
        process_tasks = list(executor.map(
            lambda f: calculate_target_path(f, target_base_dir), 
            media_files
        ))
    
    # ç¬¬äºŒæ­¥ï¼šæ‰§è¡Œæ–‡ä»¶ç§»åŠ¨ï¼ˆI/Oå¯†é›†å‹ï¼Œé€‚å½“å‡å°‘çº¿ç¨‹æ•°ï¼‰
    moved_files = 0
    skipped_count = len(skipped_files)
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=min(worker_count, 6)) as executor:
        results = executor.map(process_file, process_tasks)
        moved_files = sum(1 for result in results if result)
    
    # ç»Ÿè®¡ç»“æœ
    processed_count = len(media_files)
    error_count = processed_count - moved_files
    total_files = file_counter
    
    # æ€§èƒ½åˆ†æ
    elapsed = time.time() - start_time
    file_rate = processed_count / elapsed if elapsed > 0 else 0
    
    # æ‰“å°æ€»ç»“
    logger.info("\n" + "=" * 70)
    logger.info(f"æ•´ç†å®Œæˆ! è€—æ—¶: {elapsed:.2f}ç§’ ({file_rate:.1f}ä¸ªæ–‡ä»¶/ç§’)")
    logger.info(f"æ€»æ–‡ä»¶æ•°: {total_files}")
    logger.info(f"âœ… åª’ä½“å¤„ç†: {processed_count} (æˆåŠŸç§»åŠ¨: {moved_files})")
    logger.info(f"âš ï¸ è·³è¿‡æ–‡ä»¶: {skipped_count} (éåª’ä½“æ ¼å¼)")
    logger.info(f"âŒ å¤±è´¥å¤„ç†: {error_count}")
    logger.info("=" * 70)
    logger.info(f"ç›®æ ‡ä½ç½®: {os.path.abspath(target_base_dir)}")

def run_cli():
    """å‘½ä»¤è¡Œå…¥å£å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="åª’ä½“æ•´ç†å·¥å…· - æŒ‰æ—¥æœŸåˆ†ç±»ç…§ç‰‡å’Œè§†é¢‘",
        epilog="ç¤ºä¾‹: python organizer.py --source photos/ --target sorted/ --verbose"
    )
    parser.add_argument("--source", default=os.getcwd(), 
                        help="æºç›®å½•ï¼ˆé»˜è®¤ä¸ºå½“å‰ç›®å½•ï¼‰")
    parser.add_argument("--target", default=None, 
                        help="ç›®æ ‡ç›®å½•ï¼ˆé»˜è®¤ä¸ºæºç›®å½•ï¼‰")
    parser.add_argument("--verbose", action="store_true", 
                        help="æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—ï¼ˆè°ƒè¯•ç”¨ï¼‰")
    parser.add_argument("--workers", type=int, default=None,
                        help="å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°ï¼ˆé»˜è®¤è‡ªåŠ¨è®¾ç½®ï¼‰")
    
    args = parser.parse_args()
    
    print(f"\nğŸ“·ğŸ“¹åª’ä½“æ•´ç†å·¥å…· v2.0 - å¼€å§‹å¤„ç†...")
    print(f"æºç›®å½•: {os.path.abspath(args.source)}")
    if args.target:
        print(f"ç›®æ ‡ç›®å½•: {os.path.abspath(args.target)}")
    
    try:
        organize_media(
            source_dir=args.source,
            target_base_dir=args.target,
            verbose=args.verbose,
            max_workers=args.workers
        )
    except Exception as e:
        logger.error(f"ç¨‹åºå‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        if args.verbose:
            logger.debug(traceback.format_exc())

if __name__ == "__main__":
    run_cli()
