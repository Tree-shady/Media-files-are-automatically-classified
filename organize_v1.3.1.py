import os
import shutil
from PIL import Image
from PIL.ExifTags import TAGS
import datetime
import subprocess
import concurrent.futures
import logging
import time
from functools import lru_cache
import sys
import hashlib
import threading
from collections import defaultdict

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

# ä¼˜åŒ–å¸¸é‡
IMAGE_EXTENSIONS = ('.jpg', '.jpeg', '.png', '.heic', '.tiff', '.nef', '.cr2', '.arw', '.dng')
VIDEO_EXTENSIONS = ('.mp4', '.mov', '.avi', '.mkv', '.flv', '.wmv', '.3gp', '.m4v', '.mts')
ALL_EXTENSIONS = IMAGE_EXTENSIONS + VIDEO_EXTENSIONS

# çº¿ç¨‹å®‰å…¨çš„ç»Ÿè®¡å¯¹è±¡
class ProcessingStats:
    def __init__(self):
        self.lock = threading.Lock()
        self.files_moved = 0
        self.files_skipped = 0
        self.files_failed = 0
        self.files_processed = 0
        self.start_time = time.time()
        
    def moved(self):
        with self.lock:
            self.files_moved += 1
            self.files_processed += 1
            
    def skipped(self):
        with self.lock:
            self.files_skipped += 1
            self.files_processed += 1
            
    def failed(self):
        with self.lock:
            self.files_failed += 1
            self.files_processed += 1
            
    def get_stats(self):
        with self.lock:
            elapsed = time.time() - self.start_time
            return {
                'moved': self.files_moved,
                'skipped': self.files_skipped,
                'failed': self.files_failed,
                'processed': self.files_processed,
                'elapsed': elapsed
            }

def setup_logging(verbose=False):
    """é…ç½®æ—¥å¿—çº§åˆ«"""
    log_level = logging.DEBUG if verbose else logging.INFO
    logger.setLevel(log_level)

@lru_cache(maxsize=4096)
def get_cached_file_timestamp(filepath):
    """å¸¦ç¼“å­˜çš„æ–‡ä»¶ä¿®æ”¹æ—¶é—´è·å–ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰"""
    try:
        return os.path.getmtime(filepath)
    except Exception:
        return time.time()

@lru_cache(maxsize=4096)
def get_image_exif_date(image_path):
    """ä»å›¾ç‰‡EXIFè·å–æ—¥æœŸï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    try:
        with Image.open(image_path) as img:
            exif_data = img.getexif()
            
            if not exif_data:
                return None
            
            # é¢„å®šä¹‰çš„æ—¥æœŸæ ‡ç­¾IDåˆ—è¡¨ï¼ˆé¿å…æ¯æ¬¡å¾ªç¯æ‰€æœ‰æ ‡ç­¾ï¼‰
            date_tag_ids = {
                36867: "DateTimeOriginal",  # EXIFæ—¥æœŸæ—¶é—´åŸå§‹å€¼
                36868: "DateTimeDigitized",  # EXIFæ•°å­—åŒ–æ—¥æœŸæ—¶é—´
                
                # æ·»åŠ æ›´å¤šå¸¸è§å›¾ç‰‡æ ¼å¼çš„ç‰¹æ®Šæ—¥æœŸæ ‡ç­¾
                306: "DateTime",            # TIFF/EP æ ‡å‡†æ—¥æœŸæ—¶é—´
                32943: "DateTimeOriginal",  # Olympus RAW
            }
            
            for tag_id, tag_name in date_tag_ids.items():
                value = exif_data.get(tag_id)
                if value:
                    try:
                        # æ¸…ç†å¯èƒ½çš„æ§åˆ¶å­—ç¬¦å¹¶æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼
                        clean_value = value.strip().replace(':', '-', 2)
                        # æå–æ—¥æœŸéƒ¨åˆ†ï¼ˆå¿½ç•¥æ—¶é—´ï¼‰
                        date_str = clean_value.split()[0]
                        return datetime.datetime.strptime(date_str[:10], "%Y-%m-%d").date()
                    except (ValueError, TypeError):
                        continue
    except Exception as e:
        logger.debug(f"EXIFè¯»å–é”™è¯¯ {os.path.basename(image_path)}: {str(e)}")
    return None

@lru_cache(maxsize=2048)
def get_video_metadata_date(video_path):
    """å¸¦ç¼“å­˜çš„è§†é¢‘å…ƒæ•°æ®æ—¥æœŸè·å–ï¼Œæ”¯æŒæ›´å¤šæ ¼å¼"""
    try:
        cmd = [
            'ffprobe', '-v', 'error',
            '-show_entries', 'format_tags=creation_time :format_tags=creation_date',
            '-of', 'default=nokey=1:noprint_wrappers=1',
            video_path
        ]
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=10)
        
        if result.returncode == 0:
            # å°è¯•è§£æè¾“å‡ºä¸­çš„æ—¥æœŸå€¼ï¼ˆå¯èƒ½æœ‰å¤šä¸ªå€¼ï¼‰
            for date_str in result.stdout.splitlines():
                date_str = date_str.strip()
                if not date_str:
                    continue
                
                # å°è¯•æ‰€æœ‰å¯èƒ½çš„æ—¥æœŸæ ¼å¼
                formats = [
                    "%Y-%m-%dT%H:%M:%S",   # ISOæ ¼å¼ (GoPro/iPhone)
                    "%Y%m%d",               # ç´§å‡‘æ ¼å¼ (Sonyç›¸æœº)
                    "%Y/%m/%d %H:%M:%S",    # ç›®å½•/æ—¶é—´æ ¼å¼
                    "%d-%b-%Y",             # Nikonæ ¼å¼ (01-JAN-2023)
                    "%Y:%m:%d %H:%M:%S",    # EXIFæ ¼å¼çš„è§†é¢‘ï¼ˆæœ‰äº›APPä½¿ç”¨ï¼‰
                    "%b %d %H:%M:%S %Y"     # ç³»ç»Ÿæ—¥å¿—æ ¼å¼
                ]
                
                for fmt in formats:
                    try:
                        # æ¸…ç†ä¸è§„åˆ™å­—ç¬¦
                        clean_date = ''.join(c for c in date_str if c.isprintable())
                        # å¤„ç†å¯èƒ½çš„æ—¶åŒº/å¾®ç§’éƒ¨åˆ†
                        dt_str = clean_date.split('+')[0].split('.')[0]
                        dt = datetime.datetime.strptime(dt_str, fmt)
                        return dt.date()
                    except ValueError:
                        continue
        
        # å¦‚æœä¸Šè¿°éƒ½æ²¡æˆåŠŸï¼Œå°è¯•ä»æ–‡ä»¶åè§£ææ—¥æœŸï¼ˆå¸¸è§äºæ•°ç ç›¸æœºï¼‰
        basename = os.path.basename(video_path)
        if len(basename) >= 8 and basename[:4].isdigit() and basename[4:6].isdigit() and basename[6:8].isdigit():
            try:
                year = int(basename[0:4])
                month = int(basename[4:6])
                day = int(basename[6:8])
                return datetime.date(year, month, day)
            except ValueError:
                pass
                
    except (FileNotFoundError, subprocess.TimeoutExpired, subprocess.CalledProcessError) as e:
        logger.debug(f"è§†é¢‘æ—¥æœŸè·å–å¤±è´¥ {os.path.basename(video_path)}: {str(e)}")
    return None

def get_media_date_fast(media_path):
    """ä¼˜åŒ–çš„æ—¥æœŸè·å–ç­–ç•¥ï¼ˆå¸¦ç¼“å­˜å’Œå›é€€ï¼‰"""
    try:
        lower_path = media_path.lower()
        ext = os.path.splitext(lower_path)[1].lower()
        
        # å›¾ç‰‡æ–‡ä»¶ä¼˜å…ˆå°è¯•EXIF
        if ext in IMAGE_EXTENSIONS:
            exif_date = get_image_exif_date(media_path)
            if exif_date:
                return exif_date
        
        # è§†é¢‘æ–‡ä»¶å°è¯•è·å–å…ƒæ•°æ®
        if ext in VIDEO_EXTENSIONS:
            video_date = get_video_metadata_date(media_path)
            if video_date:
                return video_date
            
        # å°è¯•ä»æ–‡ä»¶åè§£ææ—¥æœŸï¼ˆå¦‚IMG_20230515_123456.jpgï¼‰
        basename = os.path.basename(media_path)
        for pattern in ["%Y%m%d", "%Y-%m-%d", "%Y_%m_%d"]:
            if len(basename) >= 10:
                for i in range(len(basename) - 10):
                    try:
                        dt = datetime.datetime.strptime(basename[i:i+10], pattern).date()
                        logger.debug(f"ä»æ–‡ä»¶åè§£ææ—¥æœŸ: {basename} -> {dt}")
                        return dt
                    except ValueError:
                        continue
                
        # æœ€åä½¿ç”¨ç¼“å­˜çš„æ–‡ä»¶ä¿®æ”¹æ—¶é—´
        timestamp = get_cached_file_timestamp(media_path)
        return datetime.datetime.fromtimestamp(timestamp).date()
    except Exception as e:
        logger.error(f"æ—¥æœŸè·å–é”™è¯¯ {os.path.basename(media_path)}: {str(e)}")
        return datetime.date.today()

def generate_unique_filename(target_dir, base_name, extension):
    """ç”Ÿæˆå”¯ä¸€æ–‡ä»¶åï¼ˆè§£å†³å†²çªï¼‰"""
    counter = 1
    base, ext = os.path.splitext(base_name)
    if not extension:
        extension = ext
    
    while True:
        # é¦–é€‰æ²¡æœ‰åç¼€çš„æ–°æ–‡ä»¶å
        new_filename = base + extension
        
        # å¦‚æœå¤šæ¬¡å¤±è´¥ï¼Œæ·»åŠ çŸ­å“ˆå¸Œ
        if counter > 3:
            file_hash = hashlib.md5(f"{base}{time.time()}".encode()).hexdigest()[:6]
            new_filename = f"{base}_{file_hash}{extension}"
            
        new_path = os.path.join(target_dir, new_filename)
        if not os.path.exists(new_path):
            return new_path
            
        counter += 1
        # é˜²æ­¢æ— é™å¾ªç¯
        if counter > 100:
            raise RuntimeError("æ— æ³•ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶å")

def calculate_target_path(file_info, target_base_dir, stats):
    """è®¡ç®—æ–‡ä»¶çš„ç›®æ ‡è·¯å¾„ï¼ŒåŒæ—¶æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
    filename, source_path = file_info
    
    try:
        # é¦–å…ˆæ£€æŸ¥æºæ–‡ä»¶æ˜¯å¦ä»ç„¶å­˜åœ¨
        if not os.path.exists(source_path):
            logger.warning(f"æ–‡ä»¶å·²æ¶ˆå¤±: {filename}")
            stats.skipped()
            return None
            
        media_date = get_media_date_fast(source_path)
        date_folder = media_date.strftime("%Y-%m-%d")
        target_dir = os.path.join(target_base_dir, date_folder)
        os.makedirs(target_dir, exist_ok=True)
        
        # è·å–æ–‡ä»¶æ‰©å±•å
        base, orig_ext = os.path.splitext(filename)
        extension = None
        
        # æŸ¥æ‰¾å®é™…æ–‡ä»¶æ‰©å±•åï¼ˆå¤„ç†åŒé‡æ‰©å±•åå¦‚ .jpg.txtï¼‰
        if filename.lower().endswith(ALL_EXTENSIONS):
            for ext in ALL_EXTENSIONS:
                if filename.lower().endswith(ext):
                    extension = ext
                    break
        if extension is None:
            extension = orig_ext
        
        new_filename = filename
        
        # ç”Ÿæˆå”¯ä¸€è·¯å¾„
        target_path = os.path.join(target_dir, new_filename)
        if os.path.exists(target_path):
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç›¸åŒæ–‡ä»¶ï¼ˆé˜²æ­¢ç§»åŠ¨ç›¸åŒæ–‡ä»¶ï¼‰
            if file_content_equal(source_path, target_path):
                logger.warning(f"è·³è¿‡é‡å¤æ–‡ä»¶: {filename}")
                stats.skipped()
                return None
                
            # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
            target_path = generate_unique_filename(target_dir, filename, extension)
        
        return (source_path, target_path, date_folder)
    
    except Exception as e:
        logger.error(f"è®¡ç®—è·¯å¾„å¤±è´¥ {filename}: {str(e)}")
        stats.failed()
        return None

def file_content_equal(file1, file2):
    """æ¯”è¾ƒä¸¤ä¸ªæ–‡ä»¶å†…å®¹æ˜¯å¦ç›¸åŒï¼ˆåŸºäºæ–‡ä»¶å¤§å°å’Œå“ˆå¸Œï¼‰"""
    if os.path.getsize(file1) != os.path.getsize(file2):
        return False
        
    try:
        hash1 = file_hash(file1)
        hash2 = file_hash(file2)
        return hash1 == hash2
    except Exception:
        return False

def file_hash(filepath, block_size=65536):
    """è®¡ç®—æ–‡ä»¶çš„å¿«é€Ÿå“ˆå¸Œå€¼"""
    hasher = hashlib.md5()
    with open(filepath, 'rb') as f:
        for chunk in iter(lambda: f.read(block_size), b''):
            hasher.update(chunk)
    return hasher.hexdigest()

def process_file(file_task, stats):
    """å®‰å…¨åœ°å¤„ç†å•ä¸ªæ–‡ä»¶ï¼ˆç§»åŠ¨æ“ä½œï¼‰ï¼Œæ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
    if file_task is None:
        return False
        
    source_path, target_path, date_folder = file_task
    filename = os.path.basename(source_path)
    
    try:
        # åŒé‡æ£€æŸ¥æºæ–‡ä»¶
        if not os.path.exists(source_path):
            logger.warning(f"æºæ–‡ä»¶å·²æ¶ˆå¤±: {filename}")
            stats.skipped()
            return False
            
        # ç§»åŠ¨æ–‡ä»¶
        shutil.move(source_path, target_path)
        new_filename = os.path.basename(target_path)
        logger.info(f"âœ“ å·²ç§»åŠ¨: {filename} -> {date_folder}/{new_filename}")
        stats.moved()
        return True
    except Exception as e:
        logger.error(f"âœ— ç§»åŠ¨å¤±è´¥: {filename} - é”™è¯¯: {str(e)}")
        stats.failed()
        return False

def organize_media(source_dir, target_base_dir=None, verbose=False, max_workers=None):
    """ä¸»å‡½æ•°ï¼šæŒ‰æ—¥æœŸæ•´ç†åª’ä½“æ–‡ä»¶ï¼ˆå›¾ç‰‡+è§†é¢‘ï¼‰"""
    setup_logging(verbose)
    global_stats = ProcessingStats()
    
    if target_base_dir is None:
        target_base_dir = source_dir
    
    logger.info(f"ğŸ” å¼€å§‹æ•´ç†åª’ä½“æ–‡ä»¶ @ {os.path.abspath(source_dir)}")
    
    # 1. æ‰«æåª’ä½“æ–‡ä»¶
    media_files = []
    total_size = 0
    skipped_dirs = []
    
    start_scan = time.time()
    for root, dirs, files in os.walk(source_dir):
        # è·³è¿‡ç³»ç»Ÿç›®å½•
        if os.path.basename(root).startswith('.') or os.path.basename(root) == '@eaDir':
            skipped_dirs.append(root)
            continue
            
        for file in files:
            ext = os.path.splitext(file)[1].lower()
            if ext in ALL_EXTENSIONS:
                file_path = os.path.join(root, file)
                media_files.append((file, file_path))
                try:
                    total_size += os.path.getsize(file_path)
                except:
                    # æ–‡ä»¶è®¿é—®é—®é¢˜ï¼Œè·³è¿‡
                    continue
                    
    if skipped_dirs:
        logger.debug(f"è·³è¿‡ {len(skipped_dirs)} ä¸ªç³»ç»Ÿç›®å½•")
    
    # æ²¡æœ‰æ–‡ä»¶æ—¶æå‰è¿”å›
    if not media_files:
        logger.info("æ²¡æœ‰æ‰¾åˆ°å¯å¤„ç†çš„åª’ä½“æ–‡ä»¶ï¼Œç¨‹åºé€€å‡º")
        return
        
    scan_time = time.time() - start_scan
    logger.info(f"ğŸ“Š æ‰«æå®Œæˆ! æ‰¾åˆ° {len(media_files)} ä¸ªåª’ä½“æ–‡ä»¶ ({total_size/1024/1024:.1f} MB) è€—æ—¶: {scan_time:.1f}ç§’")
    
    # 2. å¹¶è¡Œå¤„ç†è®¡ç®—ç›®æ ‡è·¯å¾„
    compute_tasks = []
    
    # è‡ªåŠ¨è®¡ç®—åˆé€‚çš„çº¿ç¨‹æ•°
    worker_count = max_workers or min(24, max(4, int(len(media_files) / 100)))
    logger.info(f"ğŸš€ å¯åŠ¨è®¡ç®—å¼•æ“ ({worker_count} çº¿ç¨‹)")
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=worker_count) as compute_executor:
        # æäº¤æ‰€æœ‰è®¡ç®—ä»»åŠ¡
        future_to_file = {}
        for file_info in media_files:
            future = compute_executor.submit(calculate_target_path, file_info, target_base_dir, global_stats)
            future_to_file[future] = file_info[0]
        
        # æ‰¹é‡è·å–ç»“æœï¼ˆå¸¦è¿›åº¦æ˜¾ç¤ºï¼‰
        processed = 0
        last_report = time.time()
        
        for future in concurrent.futures.as_completed(future_to_file):
            filename = future_to_file[future]
            try:
                task = future.result()
                compute_tasks.append(task)
            except Exception as e:
                logger.error(f"è·¯å¾„è®¡ç®—é”™è¯¯ {filename}: {str(e)}")
                global_stats.failed()
                
            processed += 1
            
            # æ¯5ç§’æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
            current_time = time.time()
            if current_time - last_report > 5 or processed == len(media_files):
                last_report = current_time
                percent = processed / len(media_files) * 100
                logger.info(f"è¿›åº¦: è®¡ç®—ç›®æ ‡è·¯å¾„ {processed}/{len(media_files)} ({percent:.1f}%)")
    
    # 3. å¹¶è¡Œå¤„ç†æ–‡ä»¶ç§»åŠ¨
    # è¿‡æ»¤æ— æ•ˆä»»åŠ¡
    valid_tasks = [t for t in compute_tasks if t is not None]
    if len(valid_tasks) != len(media_files):
        diff = len(media_files) - len(valid_tasks)
        logger.warning(f"âš ï¸ è·³è¿‡ {diff} ä¸ªæ— æ³•è®¡ç®—ç›®æ ‡ä½ç½®çš„æ–‡ä»¶")
    
    if not valid_tasks:
        logger.info("æ²¡æœ‰æœ‰æ•ˆçš„æ–‡ä»¶éœ€è¦ç§»åŠ¨")
        return
        
    logger.info(f"ğŸ”„ å¼€å§‹ç§»åŠ¨ {len(valid_tasks)} ä¸ªæ–‡ä»¶...")
    
    # I/Oæ“ä½œä½¿ç”¨è¾ƒå°‘çº¿ç¨‹
    io_workers = min(worker_count, 8)
    logger.info(f"ğŸšš å¯åŠ¨æ–‡ä»¶è½¬ç§» ({io_workers} çº¿ç¨‹)")
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=io_workers) as move_executor:
        io_futures = []
        for task in valid_tasks:
            future = move_executor.submit(process_file, task, global_stats)
            io_futures.append(future)
            
        # ç­‰å¾…æ‰€æœ‰æ“ä½œå®Œæˆï¼ˆç®€å•æ–¹å¼ï¼‰
        for future in concurrent.futures.as_completed(io_futures):
            # ç»“æœå¤„ç†åœ¨submitå›è°ƒä¸­å®Œæˆ
            pass
    
    # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
    stats = global_stats.get_stats()
    elapsed = stats['elapsed']
    file_rate = stats['processed'] / elapsed if elapsed > 0 else 0
    mb_rate = total_size / (1024 * 1024) / elapsed if elapsed > 0 else 0
    
    logger.info("\n" + "=" * 60)
    logger.info(f"â­ æ•´ç†å®Œæˆ! æ€»è€—æ—¶: {elapsed:.1f}ç§’")
    logger.info(f"ğŸ“Š æ€§èƒ½: {file_rate:.1f} æ–‡ä»¶/ç§’, {mb_rate:.1f} MB/ç§’")
    logger.info(f"âœ… æˆåŠŸç§»åŠ¨: {stats['moved']} ä¸ªæ–‡ä»¶")
    logger.info(f"âš ï¸ è·³è¿‡æ–‡ä»¶: {stats['skipped']} ä¸ª")
    if stats['failed'] > 0:
        logger.info(f"âŒ å¤„ç†å¤±è´¥: {stats['failed']} ä¸ªæ–‡ä»¶")
    logger.info("=" * 60)
    logger.info(f"ğŸ ç›®æ ‡ä½ç½®: {os.path.abspath(target_base_dir)}")

def run_cli():
    """å‘½ä»¤è¡Œå…¥å£å‡½æ•°ï¼ˆå¸¦é«˜çº§é€‰é¡¹ï¼‰"""
    import argparse
    
    parser = argparse.ArgumentParser(
        formatter_class=argparse.RawDescriptionHelpFormatter,
        description="ğŸ“¸ğŸ¥ åª’ä½“æ•´ç†å·¥å…· v5.0 - é«˜æ€§èƒ½å¤šçº¿ç¨‹åˆ†ç±»å¼•æ“",
        epilog="""é«˜çº§ç”¨æ³•:
  æ‰¹é‡å¤„ç†åµŒå¥—ç›®å½•: 
    python media_organizer.py --source archive/
  è¶…å¤§æ•°æ®é›†ä¼˜åŒ–:
    python media_organizer.py --source photos/ --workers 16
  ç½‘ç»œå­˜å‚¨åŠ é€Ÿ:
    python media_organizer.py --source /mnt/nas/photos --target /ssd/temp_sorted
  è°ƒè¯•æ¨¡å¼:
    python media_organizer.py --source raw_files/ --verbose"""
    )
    parser.add_argument("--source", default=os.getcwd(), 
                        help="æºç›®å½•ï¼ˆé»˜è®¤ä¸ºå½“å‰ç›®å½•ï¼‰", metavar="PATH")
    parser.add_argument("--target", default=None, 
                        help="ç›®æ ‡ç›®å½•ï¼ˆé»˜è®¤åœ¨æºç›®å½•ä¸­æ•´ç†ï¼‰", metavar="PATH")
    parser.add_argument("--verbose", action="store_true", 
                        help="æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—ï¼ˆè°ƒè¯•ç”¨ï¼‰")
    parser.add_argument("--workers", type=int, default=8,
                        help="å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°ï¼ˆé»˜è®¤8ï¼‰", metavar="N")
    
    args = parser.parse_args()
    
    print(f"\n{'='*60}")
    print(f"ğŸ“·ğŸ“¹ åª’ä½“æ•´ç†å·¥å…· v5.0 - é«˜æ€§èƒ½åˆ†ç±»å¼•æ“")
    print(f"{'='*60}")
    print(f"æºç›®å½•  : {os.path.abspath(args.source)}")
    print(f"ç›®æ ‡ç›®å½•: {os.path.abspath(args.target if args.target else args.source)}")
    print(f"å¹¶è¡Œçº¿ç¨‹: {args.workers}")
    print(f"è¯¦ç»†æ¨¡å¼: {'æ˜¯' if args.verbose else 'å¦'}")
    print(f"{'='*60}\n")
    
    try:
        organize_media(
            source_dir=args.source,
            target_base_dir=args.target,
            verbose=args.verbose,
            max_workers=args.workers
        )
    except KeyboardInterrupt:
        print("\næ“ä½œè¢«ç”¨æˆ·ä¸­æ–­!")
        sys.exit(1)
    except Exception as e:
        logger.error(f"ç¨‹åºå‘ç”Ÿè‡´å‘½é”™è¯¯: {str(e)}", exc_info=args.verbose)
        sys.exit(1)

if __name__ == "__main__":
    run_cli()
